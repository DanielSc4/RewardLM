{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime test notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## dependencies for colab\n",
    "# !git clone https://__TOKEN_GIT__:@github.com/DanielSc4/RL-on-LM.git\n",
    "# %cd RL-on-LM/\n",
    "# !pip install -r requirements.txt\n",
    "# from huggingface_hub import login\n",
    "# login(token = '')  # https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rewardlm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon GPU)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/daniel/.cache/huggingface/datasets/allenai___json/allenai--real-toxicity-prompts-eb8779dd2693db47/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x105f81220>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rewardlm.data.data_utils import gen_benchmark_data\n",
    "from rewardlm.core.GenerativeModel import GenerativeModel\n",
    "\n",
    "generative_manager = GenerativeModel(model_id = 'EleutherAI/pythia-70m')\n",
    "\n",
    "gen_benchmark_data(generative_manager.tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rewardlm.core.RewardModel import RewardModel\n",
    "reward_manager = RewardModel(model_id = 'facebook/roberta-hate-speech-dynabench-r4-target', device='mps')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 31414,     6,   232,   328,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_manager.tokenize_text('Hello, world!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 10993,  3645,     2,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1],\n",
       "        [    0, 10815,  1181,  3645,     2,     1,     1,     1,     1,     1,\n",
       "             1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying w/ batch text\n",
    "reward_manager.tokenize_text(\n",
    "    ['First sentence', 'second longer sentence'],\n",
    "    padding = 'max_length', \n",
    "    max_length = 12, \n",
    "    truncation = True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.00119799], dtype=float32), array([0.00028152], dtype=float32))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_manager.get_score_pair(\n",
    "    prompt = reward_manager.tokenize_text('This is a prompt'),\n",
    "    response = reward_manager.tokenize_text('This is the continuation of the prompt')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# trying w/ batch text\u001b[39;00m\n\u001b[1;32m      6\u001b[0m batch_input \u001b[39m=\u001b[39m reward_manager\u001b[39m.\u001b[39mtokenize_text(\n\u001b[1;32m      7\u001b[0m     [\u001b[39m'\u001b[39m\u001b[39mFirst sentence\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msecond longer sentence\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mthird one\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m      8\u001b[0m     padding \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m      9\u001b[0m     max_length \u001b[39m=\u001b[39m \u001b[39m12\u001b[39m, \n\u001b[1;32m     10\u001b[0m     truncation \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m dataset \u001b[39m=\u001b[39m ToxicityGeneratedSet(pd\u001b[39m.\u001b[39;49mDataFrame)\n\u001b[1;32m     15\u001b[0m reward_manager\u001b[39m.\u001b[39mget_batch_score_pair(DataLoader(batch_input, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'tokenizer'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from rewardlm.data.CustomDatasets import ToxicityGeneratedSet\n",
    "import pandas as pd\n",
    "\n",
    "# trying w/ batch text\n",
    "batch_input = reward_manager.tokenize_text(\n",
    "    ['First sentence', 'second longer sentence', 'third one'],\n",
    "    padding = 'max_length', \n",
    "    max_length = 12, \n",
    "    truncation = True,\n",
    ")\n",
    "\n",
    "dataset = ToxicityGeneratedSet(pd.DataFrame)\n",
    "        \n",
    "reward_manager.get_batch_score_pair(\n",
    "    DataLoader(batch_input, batch_size=1)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /Users/daniel/miniforge3/envs/torch_new/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "CUDA SETUP: Loading binary /Users/daniel/miniforge3/envs/torch_new/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n",
      "dlopen(/Users/daniel/miniforge3/envs/torch_new/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so, 0x0006): tried: '/Users/daniel/miniforge3/envs/torch_new/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file), '/System/Volumes/Preboot/Cryptexes/OS/Users/daniel/miniforge3/envs/torch_new/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (no such file), '/Users/daniel/miniforge3/envs/torch_new/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/miniforge3/envs/torch_new/lib/python3.9/site-packages/bitsandbytes/cextension.py:33: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/Users/daniel/miniforge3/envs/torch_new/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py:221: UserWarning: No dataset is provided. Make sure to set config.batch_size to the correct value before training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch n: 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.17it/s]\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "1it [00:13, 13.69s/it]\n"
     ]
    }
   ],
   "source": [
    "from rewardlm.core.RL.RLModel import RLModel\n",
    "import torch\n",
    "\n",
    "# sample data (thanks chatGPT for the help)\n",
    "sample_data = [\n",
    "    \"I love to eat pizza.\",\n",
    "    \"The cat is sleeping.\",\n",
    "    \"She plays the guitar.\",\n",
    "    \"He is a good friend.\",\n",
    "    \"The sun is shining.\",\n",
    "    \"They went to the park.\",\n",
    "    \"I enjoy reading books.\",\n",
    "    \"We love going to movies.\",\n",
    "    \"The dog is barking.\",\n",
    "    \"She loves to dance.\",\n",
    "    \"He is a great chef.\",\n",
    "    \"The flowers are blooming.\",\n",
    "    \"They won the game.\",\n",
    "    \"I need a cup of coffee.\",\n",
    "    \"She sings beautifully.\",\n",
    "    \"He likes to swim.\",\n",
    "    \"The rain is pouring.\",\n",
    "    \"They are best friends.\",\n",
    "    \"I like to go hiking.\",\n",
    "    \"She runs every morning.\",\n",
    "]\n",
    "\n",
    "rlmanager = RLModel(\n",
    "    model_id = 'EleutherAI/pythia-70m',\n",
    "    reward_model_id = 'facebook/roberta-hate-speech-dynabench-r4-target',\n",
    "    optimized = True,\n",
    "    # log_method = 'wandb',\n",
    "    bs = 16,     # TMP TODO, Remove it when done tesing on sample data (defaults to 256)\n",
    "    accelerator_kwargs = {\n",
    "        'cpu': False if torch.cuda.is_available() else True,        # only for M1 testing\n",
    "    },\n",
    ")\n",
    "custom_prompt = 'Someone said {prompt}'\n",
    "\n",
    "dataset = rlmanager.generate_dataset(\n",
    "    text = sample_data,\n",
    "    max_len = 50,\n",
    "    custom_prompt = custom_prompt,\n",
    ")\n",
    "\n",
    "assert rlmanager.generator_manager.tokenizer.decode(dataset[0]['input_ids'], skip_special_tokens = True) == custom_prompt.format(prompt = sample_data[0]), 'Something wrong'\n",
    "\n",
    "\n",
    "history = rlmanager.train_PPO(dataset = dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ba86a85a7d4bfeac1773c31e5abe21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/5.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/Abirate--english_quotes to /Users/daniel/.cache/huggingface/datasets/Abirate___json/Abirate--english_quotes-6e72855d06356857/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1293d1c390f488f98f2958671e0ce74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e0bc7eddc24a5b869d1d3ed88f1882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/647k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bac2dcbe55a40a78f5749d7f11b9e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b69ebe82454f459236a796f2f511d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /Users/daniel/.cache/huggingface/datasets/Abirate___json/Abirate--english_quotes-6e72855d06356857/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a23409770b674e57a8a36f72d501ecbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['quote', 'author', 'tags'],\n",
      "    num_rows: 2508\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce509ad75017494aac1051609fd29287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2508 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['quote', 'author', 'tags', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 2508\n",
      "})\n",
      "trainable params: 0 || all params 331196416 || trainable(%): 0.00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccadbea9470845e2b17f57ad257b3c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 25 at dim 1 (got 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 56\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mprint\u001b[39m(data[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     49\u001b[0m \u001b[39m# custom_prompt = 'Question: {prompt}'\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39m# dataset = PromptDataset_CLM(\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39m#     tokenizer = generator_manager.tokenizer,\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39m#     text = sample_data,\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39m#     custom_prompt = custom_prompt,\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39m# )\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m generator_manager\u001b[39m.\u001b[39;49mfine_tune(\n\u001b[1;32m     57\u001b[0m     torch_dataset \u001b[39m=\u001b[39;49m data[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[1;32m     58\u001b[0m     optimized \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m     59\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Work/RewardLM/rewardlm/core/GenerativeModel.py:163\u001b[0m, in \u001b[0;36mfine_tune\u001b[0;34m(self, torch_dataset, optimized)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_trainable_parameters()\n\u001b[1;32m    146\u001b[0m \u001b[39m## Training\u001b[39;00m\n\u001b[1;32m    148\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m    149\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel,\n\u001b[1;32m    150\u001b[0m     train_dataset \u001b[39m=\u001b[39m torch_dataset,\n\u001b[1;32m    151\u001b[0m     args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m    152\u001b[0m         per_device_train_batch_size \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m,\n\u001b[1;32m    153\u001b[0m         gradient_accumulation_steps \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m,\n\u001b[1;32m    154\u001b[0m         warmup_steps \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m,\n\u001b[1;32m    155\u001b[0m         max_steps \u001b[39m=\u001b[39m \u001b[39m200\u001b[39m,\n\u001b[1;32m    156\u001b[0m         optim \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39madamw_torch\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    157\u001b[0m         learning_rate \u001b[39m=\u001b[39m \u001b[39m2e-4\u001b[39m,\n\u001b[1;32m    158\u001b[0m         fp16 \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m \u001b[39mif\u001b[39;00m optimized \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    159\u001b[0m         logging_steps \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[1;32m    160\u001b[0m         output_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./checkpoints/fine_tune/\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    161\u001b[0m     ),\n\u001b[1;32m    162\u001b[0m     data_collator \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39mDataCollatorForLanguageModeling(torch_dataset\u001b[39m.\u001b[39mtokenizer, mlm \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 163\u001b[0m )\n\u001b[1;32m    165\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m    166\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_cache \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/torch_new/lib/python3.9/site-packages/transformers/trainer.py:1664\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1661\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1662\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1663\u001b[0m )\n\u001b[0;32m-> 1664\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1665\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1666\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1667\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1668\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1669\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/torch_new/lib/python3.9/site-packages/transformers/trainer.py:1909\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1906\u001b[0m     rng_to_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1908\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 1909\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1910\u001b[0m     total_batched_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1911\u001b[0m     \u001b[39mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[0;32m~/miniforge3/envs/torch_new/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniforge3/envs/torch_new/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch_new/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch_new/lib/python3.9/site-packages/transformers/data/data_collator.py:70\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[0;34m(features, return_tensors)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39m# have the same attributes.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39m# on the whole batch.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39mif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_default_data_collator(features)\n\u001b[1;32m     71\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mreturn\u001b[39;00m tf_default_data_collator(features)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch_new/lib/python3.9/site-packages/transformers/data/data_collator.py:136\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m    134\u001b[0m             batch[k] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39mstack([f[k] \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m features]))\n\u001b[1;32m    135\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m             batch[k] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([f[k] \u001b[39mfor\u001b[39;49;00m f \u001b[39min\u001b[39;49;00m features])\n\u001b[1;32m    138\u001b[0m \u001b[39mreturn\u001b[39;00m batch\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 25 at dim 1 (got 16)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from rewardlm.core.GenerativeModel import GenerativeModel\n",
    "from rewardlm.data.CustomDatasets import PromptDataset_CLM\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "\n",
    "sample_data = [\n",
    "    \"I love to eat pizza.\",\n",
    "    \"The cat is sleeping.\",\n",
    "    \"She plays the guitar.\",\n",
    "    \"He is a good friend.\",\n",
    "    \"The sun is shining.\",\n",
    "    \"They went to the park.\",\n",
    "    \"I enjoy reading books.\",\n",
    "    \"We love going to movies.\",\n",
    "    \"The dog is barking.\",\n",
    "    \"She loves to dance.\",\n",
    "    \"He is a great chef.\",\n",
    "    \"The flowers are blooming.\",\n",
    "    \"They won the game.\",\n",
    "    \"I need a cup of coffee.\",\n",
    "    \"She sings beautifully.\",\n",
    "    \"He likes to swim.\",\n",
    "    \"The rain is pouring.\",\n",
    "    \"They are best friends.\",\n",
    "    \"I like to go hiking.\",\n",
    "    \"She runs every morning.\",\n",
    "]\n",
    "\n",
    "model_id = 'facebook/opt-350m'\n",
    "# model_id = 'EleutherAI/pythia-70m'\n",
    "generator_manager = GenerativeModel(\n",
    "    model_id,\n",
    "    # load_dtype = '8-bit',\n",
    "    accelerator_kwargs = {\n",
    "        'cpu': False if torch.cuda.is_available() else True,        # only for M1 testing\n",
    "    },\n",
    ")\n",
    "\n",
    "### Temp for test\n",
    "from datasets import load_dataset\n",
    "data = load_dataset(\"Abirate/english_quotes\")\n",
    "print(data['train'])\n",
    "\n",
    "data = data.map(lambda samples: generator_manager.tokenizer(samples['quote']), batched=True)\n",
    "\n",
    "print(data['train'])\n",
    "\n",
    "# custom_prompt = 'Question: {prompt}'\n",
    "# dataset = PromptDataset_CLM(\n",
    "#     tokenizer = generator_manager.tokenizer,\n",
    "#     text = sample_data,\n",
    "#     custom_prompt = custom_prompt,\n",
    "# )\n",
    "\n",
    "generator_manager.fine_tune(\n",
    "    torch_dataset = data['train'], \n",
    "    optimized = False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
