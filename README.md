# ðŸ¥ž **RewardLM**
Reward a Language Model with pancakes ðŸ¥ž


## **ToxicityMeter**
Toxicity meter allows measuring the toxicity of generative models based on the output of a classifier ([RoBERTa for hate speech](https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target))

Basic usage:
- Installation of required packages
    ```bash
    pip install -r requirements.txt
    ```
TBD