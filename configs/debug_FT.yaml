# Debug configuration for fine-tuning a model
# (using small size models and small batchsize to compute faster)
generation:
  model_id: MBZUAI/LaMini-GPT-124M
  
  # other model to use next for bigger trials
  # model_id = 'facebook/opt-350m'
  # model_id = 'MBZUAI/LaMini-GPT-774M'
  custom_prompt:
    user_name: 'User:'
    bot_name: 'Assistant:'
  # temp generation config: TODO: find the best one
  generation_config:
    max_new_tokens: 256
    min_new_tokens: 4
    num_beams: 4
    early_stopping: yes
    temperature: 0.8
    top_p: 0.75
    top_k: 40

fine_tune_args:
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 2
  warmup_steps: 100
  num_train_epochs: 4
  learning_rate: 0.0003
  optim: 'adamw_torch'
  save_strategy: "steps"
  save_steps: 200
  auto_find_batch_size: yes
  use_mps_device: no
  logging_strategy: 'steps'
  logging_steps: 10
  save_total_limit: 4
  report_to: wandb

data:
  subset: yes   # Used for debug. If no, the following will be ignored
  subset_size: 30