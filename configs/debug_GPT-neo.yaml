# Debug configuration (general purpose)
# EleutherAI/gpt-neo-125m

model_id: EleutherAI/gpt-neo-125m
debug: yes                # CHANGE (prints output)

generation:  
  custom_prompt:
    # from LLaMA
    user_name: '### Question:'
    bot_name: '### Answer:'
  # temp generation config: TODO: find the best one
  generation_config:
    max_new_tokens: 256
    min_new_tokens: 4
    do_sample: yes
    num_beams: 4
    early_stopping: no
    temperature: 0.7
    top_p: 0.7
    top_k: 50
  
RL_args:
  reward_model_id: facebook/roberta-hate-speech-dynabench-r4-target
  PPO_config:
    learning_rate: 0.000005
    log_with: null        # CHANGE w/ wandb
    mini_batch_size: 2    # (16) mini batch size (<= batch size) for each model forward pass 
    batch_size: 4         # (256) batch size used for each optimization step
    gradient_accumulation_steps: 1
    
LoRA_config:
  r: 32                   # the rank of the update matrices. Lower -> fewer trainable params
  lora_alpha: 32          # LoRA scaling factor
  target_modules:
    - 'q_proj'
    - 'v_proj'
  lora_dropout: 0.05      # 
  bias: 'none'            # if the bias parameters should be trained ['none', 'lora_only', 'all']
  task_type: 'CAUSAL_LM'  # task

fine_tune_args:
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 2
  warmup_steps: 100
  num_train_epochs: 4
  learning_rate: 0.0003
  optim: 'adamw_torch'
  save_strategy: "steps"
  save_steps: 200
  auto_find_batch_size: yes
  use_mps_device: no
  logging_strategy: 'steps'
  logging_steps: 10
  save_total_limit: 4
  report_to: wandb

data:
  subset: no   # Used for debug. If no, the following will be ignored
  subset_size: 30



# GPTNeoForCausalLM(
#   (transformer): GPTNeoModel(
#     (wte): Embedding(50257, 768)
#     (wpe): Embedding(2048, 768)
#     (drop): Dropout(p=0.0, inplace=False)
#     (h): ModuleList(
#       (0-11): 12 x GPTNeoBlock(
#         (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
#         (attn): GPTNeoAttention(
#           (attention): GPTNeoSelfAttention(
#             (attn_dropout): Dropout(p=0.0, inplace=False)
#             (resid_dropout): Dropout(p=0.0, inplace=False)
#             (k_proj): Linear(in_features=768, out_features=768, bias=False)
#             (v_proj): Linear(in_features=768, out_features=768, bias=False)
#             (q_proj): Linear(in_features=768, out_features=768, bias=False)
#             (out_proj): Linear(in_features=768, out_features=768, bias=True)
#           )
#         )
#         (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
#         (mlp): GPTNeoMLP(
#           (c_fc): Linear(in_features=768, out_features=3072, bias=True)
#           (c_proj): Linear(in_features=3072, out_features=768, bias=True)
#           (act): NewGELUActivation()
#           (dropout): Dropout(p=0.0, inplace=False)
#         )
#       )
#     )
#     (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
#   )
#   (lm_head): Linear(in_features=768, out_features=50257, bias=False)
# )