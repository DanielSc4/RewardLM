# Debug configuration for RL 
# (using small size models and small batchsize to compute faster)
reward:
  model_id: facebook/roberta-hate-speech-dynabench-r4-target

generation:
  model_id: MBZUAI/LaMini-GPT-124M

PPO:
  bs: 128     # batch size used for each optimization step
  mini_bs: 4  # mini batch size (<= batch size) for each model forward pass 