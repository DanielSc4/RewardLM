# Debug configuration for RL 
# (using small size models and small batchsize to compute faster)
reward:
  model_id: facebook/roberta-hate-speech-dynabench-r4-target

generation:
  model_id: MBZUAI/LaMini-GPT-124M
  dtype: 8-bit    # for toxicity meter
  # other model to use next for bigger trials
  # model_id = 'MBZUAI/LaMini-GPT-124M'
  # model_id = 'togethercomputer/RedPajama-INCITE-Chat-3B-v1'
  # model_id = 'AlekseyKorshuk/vicuna-7b'

PPO:
  bs: 4     # batch size used for each optimization step
  mini_bs: 2  # mini batch size (<= batch size) for each model forward pass 

data:
  subset: yes   # Used for debug. If no, the following will be ignored
  subset_size: 10