{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### colab requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ONLY on colab\n",
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "# !pip install trl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â¬‡ï¸ Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer,\n",
    "    RobertaForSequenceClassification,   # reward model\n",
    "    RobertaTokenizer,                   # reward model tokenizer\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from trl import (\n",
    "    AutoModelForCausalLMWithValueHead, \n",
    "    PPOConfig, \n",
    "    PPOTrainer,\n",
    "    create_reference_model,\n",
    "    set_seed,\n",
    ")\n",
    "from trl.core import LengthSampler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'ybelkada/gpt-j-6b-sharded-bf16'       # has 28 layers\n",
    "model_name = 'EleutherAI/gpt-neo-125m'              # smallest one, having 12 layers\n",
    "log_with = None                     # or 'wandb'\n",
    "learning_rate = (1.47e-5) * 2       # default one (?)\n",
    "mini_batch_size = 4                 # PPO minibatch size\n",
    "batch_size = 12                     # batchsize (for the model?)\n",
    "ppo_epochs = 100\n",
    "gradient_accumulation_steps = 1     # [TODO]: check what is this!\n",
    "model_save_path = './checkpoints/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig(\n",
    "    model_name = model_name,\n",
    "    learning_rate = learning_rate,\n",
    "    log_with = log_with,\n",
    "    ppo_epochs = ppo_epochs,\n",
    "    mini_batch_size = mini_batch_size,\n",
    "    batch_size = batch_size,\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Method that loads the dataset, customize it to train on different data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/daniel/.cache/huggingface/datasets/allenai___json/allenai--real-toxicity-prompts-eb8779dd2693db47/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
      "Loading cached processed dataset at /Users/daniel/.cache/huggingface/datasets/allenai___json/allenai--real-toxicity-prompts-eb8779dd2693db47/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e/cache-98c7dae678df6ade.arrow\n",
      "Loading cached processed dataset at /Users/daniel/.cache/huggingface/datasets/allenai___json/allenai--real-toxicity-prompts-eb8779dd2693db47/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e/cache-85cd25deec90f51a.arrow\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(\n",
    "    config, \n",
    "    dataset_name=\"allenai/real-toxicity-prompts\", \n",
    "    input_min_text_length = 10, \n",
    "    input_max_text_length = 15,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset` and returns \n",
    "    a Dataloader class (from torch.utils.data)\n",
    "\n",
    "    Args:\n",
    "        dataset_name (`str`):\n",
    "            The name of the dataset to be loaded. (from load_dataset)\n",
    "    Returns:\n",
    "        dataloader (`torch.utils.data.DataLoader`):\n",
    "            The dataloader for the dataset.\n",
    "    \"\"\"\n",
    "    # from the PPO trainer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    ds = load_dataset(dataset_name, split = \"train\")\n",
    "\n",
    "    # function to filter only prompts that lead to a toxicity score > 0.3\n",
    "    # in order to increase the chances for the model to generate toxicity prompts\n",
    "    def filter_fn(sample):\n",
    "        toxicity = sample[\"prompt\"][\"toxicity\"]\n",
    "        return toxicity is not None and toxicity > 0.3\n",
    "\n",
    "    ds = ds.filter(filter_fn, batched=False)\n",
    "    \n",
    "    # only prompts in between input_min_text_length : input_max_text_length\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        prompt = sample[\"prompt\"][\"text\"]\n",
    "        continuation = sample[\"continuation\"][\"text\"]\n",
    "\n",
    "        sample[\"input_ids\"] = tokenizer.encode(prompt + continuation)[: input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    # tokenize\n",
    "    ds = ds.map(tokenize, batched = False)\n",
    "    # set as torch.utils.data.DataLoader\n",
    "    ds.set_format(type=\"torch\")\n",
    "\n",
    "    ds = ds.train_test_split(test_size = 0.2, shuffle = False)[\"train\"]\n",
    "\n",
    "    return ds\n",
    "\n",
    "# Getting dataset\n",
    "min_input_length = 30\n",
    "max_input_length = 40\n",
    "dataset = build_dataset(\n",
    "    config, \n",
    "    input_min_text_length = min_input_length, \n",
    "    input_max_text_length = max_input_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other stuff\n",
    "# set seed before initializing value head for deterministic eval\n",
    "set_seed(config.seed)\n",
    "\n",
    "# dunno\n",
    "def collator(data):\n",
    "    return dict(\n",
    "        (key, [d[key] for d in data]) for key in data[0]\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference model\n",
    "We load the model in bfloat16 to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model loaded in float16 to reduce memory usage\n",
    "model_stock = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name, \n",
    "    torch_dtype = torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(model_stock)\n",
    "\n",
    "# GPT-2 / GPT-J tokenizer has a pad token, but it is not eos_token by default. We need to set it to eos_token.\n",
    "# only for this model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "## so ONLY FOR GPT (?)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# reference model sharing 5 layers out of 12 total layers (for 'gpt-neo-125m')\n",
    "# reference models are frozen copies of the model that is trained (in eval mode) [from doc]\n",
    "ref_model = create_reference_model(\n",
    "    model, \n",
    "    num_shared_layers = 5,\n",
    "    pattern = 'transformer.h.{layer}'\n",
    ")\n",
    "\n",
    "# We make sure to use `Adam` optimizer on the model parameters that require gradients.\n",
    "optimizer = Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), \n",
    "    lr = config.learning_rate\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![shared layers](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/trl-shared-layers.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_trainer = PPOTrainer(\n",
    "    config,\n",
    "    model,                          # model to be optimized w/ value head\n",
    "    ref_model = ref_model,          # reference model used for KL penalty\n",
    "    tokenizer = tokenizer,          # used for encoding\n",
    "    dataset = dataset,\n",
    "    data_collator = collator,\n",
    "    optimizer = optimizer,\n",
    "    # lr_scheduler = ,              # <optional> lr scheduler\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_model_id = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "toxicity_tokenizer = RobertaTokenizer.from_pretrained(toxicity_model_id)\n",
    "# Also load the toxicity model in fp16 to save memory\n",
    "toxicity_model = RobertaForSequenceClassification.from_pretrained(\n",
    "    toxicity_model_id, \n",
    "    torch_dtype=torch.float16\n",
    ").to(ppo_trainer.accelerator.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then define the arguments to pass to the `generate` function. These arguments\n",
    "# are passed to the `generate` function of the PPOTrainer, which is a wrapper around\n",
    "# the `generate` function of the trained model.\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "output_min_length = 20\n",
    "output_max_length = 30\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTORCH_ENABLE_MPS_FALLBACK=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS does not support cumsum op with int64 input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m     gen_len \u001b[39m=\u001b[39m output_length_sampler()\n\u001b[1;32m      8\u001b[0m     generation_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmax_new_tokens\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m gen_len\n\u001b[0;32m----> 9\u001b[0m     response \u001b[39m=\u001b[39m ppo_trainer\u001b[39m.\u001b[39;49mgenerate(query, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgeneration_kwargs)\n\u001b[1;32m     10\u001b[0m     response_tensors\u001b[39m.\u001b[39mappend(response\u001b[39m.\u001b[39msqueeze()[\u001b[39m-\u001b[39mgen_len:])\n\u001b[1;32m     11\u001b[0m batch[\u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m [tokenizer\u001b[39m.\u001b[39mdecode(r\u001b[39m.\u001b[39msqueeze()) \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m response_tensors]\n",
      "File \u001b[0;32m~/miniforge3/envs/torch_new/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py:374\u001b[0m, in \u001b[0;36mPPOTrainer.generate\u001b[0;34m(self, query_tensor, **generation_kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\u001b[39mself\u001b[39m, query_tensor: torch\u001b[39m.\u001b[39mTensor, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgeneration_kwargs):\n\u001b[1;32m    361\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[39m    Generate response with the model given the query tensor.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39m    call the `generate` method of the model.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[39m        `torch.LongTensor`: A tensor of shape (`batch_size`, `gen_len`) containing response tokens.\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 374\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49munwrap_model(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel)\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    375\u001b[0m         input_ids\u001b[39m=\u001b[39;49mquery_tensor\u001b[39m.\u001b[39;49munsqueeze(dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgeneration_kwargs\n\u001b[1;32m    376\u001b[0m     )\n\u001b[1;32m    378\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniforge3/envs/torch_new/lib/python3.9/site-packages/trl/models/modeling_value_head.py:195\u001b[0m, in \u001b[0;36mAutoModelForCausalLMWithValueHead.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    184\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39m    A simple wrapper around the `generate` method of the wrapped model.\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39m    Please refer to the [`generate`](https://huggingface.co/docs/transformers/internal/generation_utils)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39m            Keyword arguments passed to the `generate` method of the wrapped model.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpretrained_model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch_new/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch_new/lib/python3.9/site-packages/transformers/generation/utils.py:1452\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1444\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1445\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1446\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1447\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1448\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1449\u001b[0m     )\n\u001b[1;32m   1451\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1452\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1453\u001b[0m         input_ids,\n\u001b[1;32m   1454\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1455\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1456\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1457\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1458\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1459\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1460\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1461\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1462\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1463\u001b[0m     )\n\u001b[1;32m   1465\u001b[0m \u001b[39melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1466\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m generation_config\u001b[39m.\u001b[39mnum_beams:\n",
      "File \u001b[0;32m~/miniforge3/envs/torch_new/lib/python3.9/site-packages/transformers/generation/utils.py:2465\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2462\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   2464\u001b[0m \u001b[39m# prepare model inputs\u001b[39;00m\n\u001b[0;32m-> 2465\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   2467\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m   2468\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[1;32m   2469\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[1;32m   2470\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   2471\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   2472\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   2473\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/torch_new/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:697\u001b[0m, in \u001b[0;36mGPTNeoForCausalLM.prepare_inputs_for_generation\u001b[0;34m(self, input_ids, past_key_values, **kwargs)\u001b[0m\n\u001b[1;32m    693\u001b[0m position_ids \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mposition_ids\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    695\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m position_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    696\u001b[0m     \u001b[39m# create position_ids on the fly for batch generation\u001b[39;00m\n\u001b[0;32m--> 697\u001b[0m     position_ids \u001b[39m=\u001b[39m attention_mask\u001b[39m.\u001b[39;49mlong()\u001b[39m.\u001b[39;49mcumsum(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    698\u001b[0m     position_ids\u001b[39m.\u001b[39mmasked_fill_(attention_mask \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m    699\u001b[0m     \u001b[39mif\u001b[39;00m past_key_values:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS does not support cumsum op with int64 input"
     ]
    }
   ],
   "source": [
    "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    # Get response from the policy model\n",
    "    response_tensors = []\n",
    "    for query in query_tensors:\n",
    "        gen_len = output_length_sampler()\n",
    "        generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "        response = ppo_trainer.generate(query, **generation_kwargs)\n",
    "        response_tensors.append(response.squeeze()[-gen_len:])\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "    print('Response obtained')\n",
    "\n",
    "    # Compute sentiment score # noqa\n",
    "    texts = batch[\"response\"]\n",
    "    print('Asking toxicity')\n",
    "    toxicity_inputs = toxicity_tokenizer(\n",
    "        texts, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    ).to(ppo_trainer.accelerator.device)\n",
    "    logits = toxicity_model(**toxicity_inputs).logits.float()\n",
    "    toxicity_labels = (logits[:, 0]).tolist()\n",
    "    print('Got toxicity')\n",
    "\n",
    "    rewards = [torch.tensor(output) for output in toxicity_labels]\n",
    "    print('Reward computed, running PPO')\n",
    "\n",
    "    # Run PPO step\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)\n",
    "    print('PPO updated')\n",
    "\n",
    "    # Save model every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        if ppo_trainer.accelerator.is_main_process:\n",
    "            ppo_trainer.save_pretrained(model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0753d7e52e1f9804d15a82703eeadfc1eaa710dcb618d460ad307da94d897d93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
