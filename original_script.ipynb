{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    RobertaForSequenceClassification,\n",
    "    RobertaTokenizer,\n",
    ")\n",
    "\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, create_reference_model, set_seed\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "########################################################################\n",
    "# This is a fully working simple example to use trl with accelerate.\n",
    "#\n",
    "# This example fine-tunes a GPTJ model to generate less toxic contents\n",
    "# by using allenai/real-toxicity-prompts dataset. We use PPO\n",
    "#  (proximal policy optimization) to optimize the model.\n",
    "# in any of the following settings (with the same script):\n",
    "#   - single CPU or single GPU\n",
    "#   - multi GPUS (using PyTorch distributed mode)\n",
    "#   - multi GPUS (using DeepSpeed ZeRO-Offload stages 1 & 2)\n",
    "#   - fp16 (mixed-precision) or fp32 (normal precision)\n",
    "#\n",
    "# To run it in each of these various modes, first initialize the accelerate\n",
    "# configuration with `accelerate config`\n",
    "#\n",
    "########################################################################\n",
    "\n",
    "\n",
    "\n",
    "model_name = 'ybelkada/gpt-j-6b-sharded-bf16'       # has 28 layers\n",
    "model_name = 'EleutherAI/gpt-neo-125m'              # smallest one, having 12 layers\n",
    "log_with = None                     # or 'wandb'\n",
    "learning_rate = (1.47e-5) * 2       # default one (?)\n",
    "mini_batch_size = 4                 # PPO minibatch size\n",
    "batch_size = 14                     # batchsize (for the model?)\n",
    "ppo_epochs = 100\n",
    "gradient_accumulation_steps = 1     # [TODO]: check what is this!\n",
    "model_save_path = './checkpoints/'\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name = model_name,\n",
    "    learning_rate = learning_rate,\n",
    "    log_with = log_with,\n",
    "    ppo_epochs = ppo_epochs,\n",
    "    mini_batch_size = mini_batch_size,\n",
    "    batch_size = batch_size,\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Below is an example function to build the dataset. In our case, we use the IMDB dataset\n",
    "# from the `datasets` library. One should customize this function to train the model on\n",
    "# its own dataset.\n",
    "def build_dataset(\n",
    "    config, dataset_name=\"allenai/real-toxicity-prompts\", input_min_text_length=5, input_max_text_length=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "    customize this function to train the model on its own dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (`str`):\n",
    "            The name of the dataset to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        dataloader (`torch.utils.data.DataLoader`):\n",
    "            The dataloader for the dataset.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    ds = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "    def filter_fn(sample):\n",
    "        toxicity = sample[\"prompt\"][\"toxicity\"]\n",
    "        return toxicity is not None and toxicity > 0.3\n",
    "\n",
    "    ds = ds.filter(filter_fn, batched=False)\n",
    "\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        prompt = sample[\"prompt\"][\"text\"]\n",
    "        continuation = sample[\"continuation\"][\"text\"]\n",
    "\n",
    "        sample[\"input_ids\"] = tokenizer.encode(prompt + continuation)[: input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "\n",
    "    ds = ds.train_test_split(test_size=0.2, shuffle=False)[\"train\"]\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "# We retrieve the dataloader by calling the `build_dataset` function.\n",
    "min_input_length = 30\n",
    "max_input_length = 40\n",
    "dataset = build_dataset(config, input_min_text_length=min_input_length, input_max_text_length=max_input_length)\n",
    "\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "\n",
    "# set seed before initializing value head for deterministic eval\n",
    "set_seed(config.seed)\n",
    "\n",
    "# Now let's build the model, the reference model, and the tokenizer. We first load the model\n",
    "# in bfloat16 to save memory using `transformers`.\n",
    "model = AutoModelForCausalLM.from_pretrained(config.model_name, torch_dtype=torch.bfloat16)\n",
    "# And then we pass the loaded model to `AutoModelForCausalLMWithValueHead`.\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(model)\n",
    "\n",
    "# We create a reference model by sharing 20 layers\n",
    "ref_model = create_reference_model(model, num_shared_layers=20)\n",
    "\n",
    "# We make sure to use `Adam` optimizer on the model parameters that require gradients.\n",
    "optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=config.learning_rate)\n",
    "\n",
    "# GPT-2 / GPT-J tokenizer has a pad token, but it is not eos_token by default. We need to set it to eos_token.\n",
    "# only for this model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# We then build the PPOTrainer, passing the model, the reference model, the tokenizer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config,\n",
    "    model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=dataset,\n",
    "    data_collator=collator,\n",
    "    optimizer=optimizer,\n",
    ")\n",
    "\n",
    "# We then build the reward pipeline, we will use the toxicity model to compute the reward.\n",
    "# We first load the toxicity model and tokenizer.\n",
    "toxicity_model_id = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "toxicity_tokenizer = RobertaTokenizer.from_pretrained(toxicity_model_id)\n",
    "# We load the toxicity model in fp16 to save memory.\n",
    "toxicity_model = RobertaForSequenceClassification.from_pretrained(toxicity_model_id, torch_dtype=torch.float16).to(\n",
    "    ppo_trainer.accelerator.device\n",
    ")\n",
    "\n",
    "\n",
    "# We then define the arguments to pass to the `generate` function. These arguments\n",
    "# are passed to the `generate` function of the PPOTrainer, which is a wrapper around\n",
    "# the `generate` function of the trained model.\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "output_min_length = 20\n",
    "output_max_length = 30\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "model_save_path = model_save_path\n",
    "\n",
    "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    # Get response from the policy model\n",
    "    response_tensors = []\n",
    "    for query in query_tensors:\n",
    "        gen_len = output_length_sampler()\n",
    "        generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "        response = ppo_trainer.generate(query, **generation_kwargs)\n",
    "        response_tensors.append(response.squeeze()[-gen_len:])\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "    # Compute sentiment score # noqa\n",
    "    texts = batch[\"response\"]\n",
    "    toxicity_inputs = toxicity_tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(\n",
    "        ppo_trainer.accelerator.device\n",
    "    )\n",
    "    logits = toxicity_model(**toxicity_inputs).logits.float()\n",
    "    toxicity_labels = (logits[:, 0]).tolist()\n",
    "\n",
    "    rewards = [torch.tensor(output) for output in toxicity_labels]\n",
    "\n",
    "    # Run PPO step\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "    # Save model every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        if ppo_trainer.accelerator.is_main_process:\n",
    "            ppo_trainer.save_pretrained(model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c572d5174d38978ba02a3ac3c114ecb0cec161780d0cde2b670f1f68ef5adc30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
