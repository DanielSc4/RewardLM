{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## dependencies for colab\n",
    "# !git clone https://__TOKEN_GIT__:@github.com/DanielSc4/RewardLM.git\n",
    "# %cd RewardLM/\n",
    "# !pip install -r requirements.txt\n",
    "# from huggingface_hub import login\n",
    "# login(token = '')  # https://huggingface.co/settings/tokens\n",
    "\n",
    "import os\n",
    "def init():\n",
    "    # disable welcome message\n",
    "    os.environ['BITSANDBYTES_NOWELCOME'] = '1'\n",
    "    # os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "    # os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "def update_code():\n",
    "    !git pull\n",
    "    %cd RewardLM/\n",
    "\n",
    "init()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü•û Reinforcement Learning with Automatic Feedback (RLAF)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation config from [here](https://github.com/LAION-AI/Open-Assistant/blob/main/model/model_training/configs/ppo_config.yaml) seems to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('AlekseyKorshuk/vicuna-7b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/home1/p313544/Documents/RewardLM/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from rewardlm.core.RL.RLModel import RLModel\n",
    "from rewardlm.data.data_utils import get_DIALOCONAN_prepro\n",
    "from transformers import GenerationConfig\n",
    "from rewardlm.utils import load_config\n",
    "\n",
    "config = load_config('debug_RL')\n",
    "\n",
    "rlmanager = RLModel(\n",
    "    model_id = config['generation']['model_id'],\n",
    "    reward_model_id = config['reward']['model_id'],\n",
    "    optimized = True,   # use LoRA\n",
    "    bs = config['PPO']['bs'],\n",
    "    mini_bs = config['PPO']['mini_bs'],\n",
    "    # force the use of CPU on Apple Silicon devices (mps not supported):\n",
    "    accelerator_kwargs = {\n",
    "        'cpu': False if torch.cuda.is_available() else True,\n",
    "    },\n",
    "    generation_config=GenerationConfig(\n",
    "        max_new_tokens = 512,\n",
    "        min_new_tokens = 5,\n",
    "        pad_token_id = 0,       # crashes while using batchsize > 1 only on mps device if not set\n",
    "        temperature = 1,\n",
    "        top_p = .7,\n",
    "        top_k = 0,\n",
    "        do_sample = True\n",
    "        # diversity_penalty = .1, # should use num_beam_groups > 1\n",
    "    )\n",
    ")\n",
    "\n",
    "data = get_DIALOCONAN_prepro(delete_last_assistant_response = True)\n",
    "if config['data']['subset']:\n",
    "    # select only the first `subset_size` samples\n",
    "    data = data[:config['data']['subset_size']]\n",
    "dataset = rlmanager.generate_dataset(text = data)\n",
    "\n",
    "stats = rlmanager.train_PPO(dataset = dataset)\n",
    "print('Done')\n",
    "\n",
    "# assuming debug if subset is active\n",
    "if not config['data']['subset']:\n",
    "    # save trainer (model, tokenizer & config) to the hub\n",
    "    repo_id = 'DanielSc4/' + config['generation']['model_id'].split('/')[1] + '-RL-LoRA-test0'\n",
    "\n",
    "    rlmanager.push_generator_to_hub(repo_id = repo_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üë®üèº‚Äçüè´ Model fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /Users/daniel/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "CUDA SETUP: Loading binary /Users/daniel/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n",
      "dlopen(/Users/daniel/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so, 0x0006): tried: '/Users/daniel/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file), '/System/Volumes/Preboot/Cryptexes/OS/Users/daniel/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (no such file), '/Users/daniel/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file)\n",
      "Accelerator selected device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/bitsandbytes/cextension.py:33: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded in fp32 (standard) mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0216a363ead4c71910a6a994b38b0bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1179648 || all params 126378240 || trainable(%): 0.93\n",
      "Trainer device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdanielsc4\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/daniel/Documents/Work/RewardLM/wandb/run-20230623_171405-ua950ub4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/danielsc4/huggingface/runs/ua950ub4' target=\"_blank\">gpt-neo-125m-FT-LoRA-test1</a></strong> to <a href='https://wandb.ai/danielsc4/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/danielsc4/huggingface' target=\"_blank\">https://wandb.ai/danielsc4/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/danielsc4/huggingface/runs/ua950ub4' target=\"_blank\">https://wandb.ai/danielsc4/huggingface/runs/ua950ub4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "482e722aa6954af29b6311e6fe683e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GenerationConfig\n",
    "from rewardlm.data.data_utils import get_DIALOCONAN_prepro, get_dataset_CLM\n",
    "from rewardlm.core.GenerativeModel import GenerativeModel\n",
    "from rewardlm.utils import load_config\n",
    "\n",
    "config = load_config('debug_notGPT_FT')\n",
    "repo_id = 'DanielSc4/' + config['generation']['model_id'].split('/')[1] + '-FT-LoRA-test1'\n",
    "\n",
    "generator_manager = GenerativeModel(\n",
    "    config['generation']['model_id'],\n",
    "    load_dtype = '8-bit' if torch.cuda.is_available() else 'fp32',\n",
    "    # force the use of CPU on Apple Silicon devices (mps not supported):\n",
    "    generation_config=GenerationConfig(**config['generation']['generation_config']),\n",
    "    accelerator_kwargs = {\n",
    "        'cpu': False if torch.cuda.is_available() else True,\n",
    "    },\n",
    ")\n",
    "\n",
    "# download dataset\n",
    "data = get_DIALOCONAN_prepro(**config['generation']['custom_prompt'])\n",
    "if config['data']['subset']:\n",
    "    print('getting subset')\n",
    "    # select only the first `subset_size` samples\n",
    "    data = data[:config['data']['subset_size']]\n",
    "dataset = get_dataset_CLM(\n",
    "    data, \n",
    "    context_length = 512, \n",
    "    tokenizer = generator_manager.tokenizer\n",
    ")\n",
    "\n",
    "generator_manager.fine_tune(\n",
    "    dataset = dataset, \n",
    "    val_set_per=.1,     # 10% of the dataset\n",
    "    optimized = True,   # if torch.cuda.is_available() else False,\n",
    "    lr = config['generation']['lr'],\n",
    "    epochs = config['generation']['epochs'],\n",
    "    initial_bs = config['generation']['initial_bs'],\n",
    "    run_name = repo_id.split('/')[1],\n",
    ")\n",
    "\n",
    "# assuming debug if subset is active\n",
    "if not config['data']['subset']:\n",
    "    # save model to the hub\n",
    "    generator_manager.push_to_hub(repo_id = repo_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the fine tuned model from ü§ó Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerator selected device: mps\n",
      "Obtaining original model: MBZUAI/LaMini-GPT-124M\n",
      "Inference test: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS does not support cumsum op with int64 input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mInference test: \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m inputs \u001b[39m=\u001b[39m new_generator\u001b[39m.\u001b[39mtokenizer(\u001b[39m\"\u001b[39m\u001b[39mUser: Woman should always be considere inferior?\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAssistant: \u001b[39m\u001b[39m\"\u001b[39m, return_tensors \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m new_generator\u001b[39m.\u001b[39;49minference_fine_tuned(tokenized_batch\u001b[39m=\u001b[39;49minputs, return_decoded\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Documents/Work/RewardLM/rewardlm/core/GenerativeModel.py:257\u001b[0m, in \u001b[0;36mGenerativeModel.inference_fine_tuned\u001b[0;34m(self, tokenized_batch, return_decoded)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n\u001b[1;32m    256\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 257\u001b[0m     output_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtokenized_batch, generation_config \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgeneration_config)\n\u001b[1;32m    259\u001b[0m \u001b[39mif\u001b[39;00m return_decoded:\n\u001b[1;32m    260\u001b[0m     output_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mdecode(output_model[\u001b[39m0\u001b[39m], skip_special_tokens \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/peft/peft_model.py:731\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(peft_config, PromptLearningConfig):\n\u001b[0;32m--> 731\u001b[0m         outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    732\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    733\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m~/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/trl/models/modeling_value_head.py:195\u001b[0m, in \u001b[0;36mAutoModelForCausalLMWithValueHead.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    184\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39m    A simple wrapper around the `generate` method of the wrapped model.\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39m    Please refer to the [`generate`](https://huggingface.co/docs/transformers/internal/generation_utils)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39m            Keyword arguments passed to the `generate` method of the wrapped model.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpretrained_model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1522\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1517\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnum_return_sequences has to be 1 when doing greedy search, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1518\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut is \u001b[39m\u001b[39m{\u001b[39;00mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1519\u001b[0m         )\n\u001b[1;32m   1521\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1522\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[1;32m   1523\u001b[0m         input_ids,\n\u001b[1;32m   1524\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1525\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1526\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1527\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1528\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1529\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1530\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1531\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1532\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1533\u001b[0m     )\n\u001b[1;32m   1535\u001b[0m \u001b[39melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[1;32m   1536\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2336\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2333\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   2335\u001b[0m \u001b[39m# prepare model inputs\u001b[39;00m\n\u001b[0;32m-> 2336\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   2338\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m   2339\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[1;32m   2340\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[1;32m   2341\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   2342\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   2343\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   2344\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1025\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.prepare_inputs_for_generation\u001b[0;34m(self, input_ids, past_key_values, inputs_embeds, **kwargs)\u001b[0m\n\u001b[1;32m   1021\u001b[0m position_ids \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mposition_ids\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   1023\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m position_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1024\u001b[0m     \u001b[39m# create position_ids on the fly for batch generation\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m     position_ids \u001b[39m=\u001b[39m attention_mask\u001b[39m.\u001b[39;49mlong()\u001b[39m.\u001b[39;49mcumsum(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1026\u001b[0m     position_ids\u001b[39m.\u001b[39mmasked_fill_(attention_mask \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m   1027\u001b[0m     \u001b[39mif\u001b[39;00m past_key_values:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS does not support cumsum op with int64 input"
     ]
    }
   ],
   "source": [
    "from rewardlm.core.GenerativeModel import GenerativeModel\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "new_generator = GenerativeModel(\n",
    "    model_id=repo_id,\n",
    "    load_from_peft=True,\n",
    ")\n",
    "\n",
    "print('Inference test: ')\n",
    "inputs = new_generator.tokenizer(\"User: Woman should always be considere inferior?\\nAssistant: \", return_tensors = 'pt')\n",
    "\n",
    "new_generator.inference_fine_tuned(tokenized_batch=inputs, return_decoded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Explain how to solve the immigration probelem\n",
      "Assistant: \n",
      "\n",
      "To solve the immigration probelem, you need to follow these steps:\n",
      "\n",
      "1. Identify the source of the probelem.\n",
      "2. Identify the purpose of the probelem.\n",
      "3. Determine the appropriate action to take.\n",
      "\n",
      "Steps to solve the immigration probelem:\n",
      "\n",
      "1. Identify the source of the probelem.\n",
      "2. Identify the purpose of the probelem.\n",
      "3. Determine the appropriate action to take.\n"
     ]
    }
   ],
   "source": [
    "text = 'Explain how to solve the immigration problem'\n",
    "inputs = new_generator.tokenizer(\n",
    "    \"User: {text}\\nAssistant: \".format(text = text), \n",
    "    return_tensors = 'pt'\n",
    ")\n",
    "output = new_generator.inference_fine_tuned(tokenized_batch=inputs, return_decoded=True)\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚öñÔ∏è ToxicityMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /Users/daniel/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "CUDA SETUP: Loading binary /Users/daniel/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n",
      "dlopen(/Users/daniel/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so, 0x0006): tried: '/Users/daniel/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file), '/System/Volumes/Preboot/Cryptexes/OS/Users/daniel/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (no such file), '/Users/daniel/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file)\n",
      "Accelerator selected device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/bitsandbytes/cextension.py:33: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon GPU)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/daniel/.cache/huggingface/datasets/allenai___json/allenai--real-toxicity-prompts-eb8779dd2693db47/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [04:33<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m batchsize \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n\u001b[1;32m     22\u001b[0m custom_prompt \u001b[39m=\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mStatement: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{prompt}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mResponse: \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m toxicity_df \u001b[39m=\u001b[39m toxicity_meter\u001b[39m.\u001b[39;49mmeasure_toxicity(\n\u001b[1;32m     25\u001b[0m     text_prompt \u001b[39m=\u001b[39;49m get_real_toxicity_prompts()[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mto_list()[:\u001b[39m5\u001b[39;49m],\n\u001b[1;32m     26\u001b[0m     custom_prompt \u001b[39m=\u001b[39;49m custom_prompt, \n\u001b[1;32m     27\u001b[0m     batch_size \u001b[39m=\u001b[39;49m batchsize,\n\u001b[1;32m     28\u001b[0m     print_response\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     29\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Work/RewardLM/rewardlm/ToxicityMeter.py:103\u001b[0m, in \u001b[0;36mToxicityMeter.measure_toxicity\u001b[0;34m(self, text_prompt, custom_prompt, print_response, batch_size)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mfor\u001b[39;00m ele \u001b[39min\u001b[39;00m inputs:\n\u001b[1;32m    101\u001b[0m     inputs[ele] \u001b[39m=\u001b[39m inputs[ele]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerator_manager\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 103\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerator_manager\u001b[39m.\u001b[39;49minference_fine_tuned(\n\u001b[1;32m    104\u001b[0m     tokenized_batch \u001b[39m=\u001b[39;49m inputs,\n\u001b[1;32m    105\u001b[0m     return_decoded \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    106\u001b[0m )\n\u001b[1;32m    107\u001b[0m prmpt, rspns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_prompts_responses(\n\u001b[1;32m    108\u001b[0m     prompts \u001b[39m=\u001b[39m inputs[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    109\u001b[0m     responses \u001b[39m=\u001b[39m output,\n\u001b[1;32m    110\u001b[0m     print_r \u001b[39m=\u001b[39m print_response,\n\u001b[1;32m    111\u001b[0m )\n\u001b[1;32m    112\u001b[0m generation[\u001b[39m'\u001b[39m\u001b[39mprompts\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mextend(prmpt)\n",
      "File \u001b[0;32m~/Documents/Work/RewardLM/rewardlm/core/GenerativeModel.py:261\u001b[0m, in \u001b[0;36mGenerativeModel.inference_fine_tuned\u001b[0;34m(self, tokenized_batch, return_decoded)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n\u001b[1;32m    260\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 261\u001b[0m     output_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtokenized_batch, generation_config \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgeneration_config)\n\u001b[1;32m    263\u001b[0m \u001b[39mif\u001b[39;00m return_decoded:\n\u001b[1;32m    264\u001b[0m     output_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mdecode(output_model[\u001b[39m0\u001b[39m], skip_special_tokens \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1611\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1604\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1605\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1606\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   1607\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1608\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1609\u001b[0m     )\n\u001b[1;32m   1610\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1611\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[1;32m   1612\u001b[0m         input_ids,\n\u001b[1;32m   1613\u001b[0m         beam_scorer,\n\u001b[1;32m   1614\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1615\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1616\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1617\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1618\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1619\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1620\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1621\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1622\u001b[0m     )\n\u001b[1;32m   1624\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1625\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1626\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m~/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2982\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2978\u001b[0m model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   2979\u001b[0m     outputs, model_kwargs, is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder\n\u001b[1;32m   2980\u001b[0m )\n\u001b[1;32m   2981\u001b[0m \u001b[39mif\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mpast_key_values\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 2982\u001b[0m     model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mpast_key_values\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reorder_cache(model_kwargs[\u001b[39m\"\u001b[39;49m\u001b[39mpast_key_values\u001b[39;49m\u001b[39m\"\u001b[39;49m], beam_idx)\n\u001b[1;32m   2984\u001b[0m \u001b[39mif\u001b[39;00m return_dict_in_generate \u001b[39mand\u001b[39;00m output_scores:\n\u001b[1;32m   2985\u001b[0m     beam_indices \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m((beam_indices[beam_idx[i]] \u001b[39m+\u001b[39m (beam_idx[i],) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(beam_indices))))\n",
      "File \u001b[0;32m~/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:752\u001b[0m, in \u001b[0;36mGPTNeoXForCausalLM._reorder_cache\u001b[0;34m(self, past_key_values, beam_idx)\u001b[0m\n\u001b[1;32m    749\u001b[0m reordered_past \u001b[39m=\u001b[39m ()\n\u001b[1;32m    750\u001b[0m \u001b[39mfor\u001b[39;00m layer_past \u001b[39min\u001b[39;00m past_key_values:\n\u001b[1;32m    751\u001b[0m     reordered_past \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m--> 752\u001b[0m         \u001b[39mtuple\u001b[39;49m(past_state\u001b[39m.\u001b[39;49mindex_select(\u001b[39m0\u001b[39;49m, beam_idx) \u001b[39mfor\u001b[39;49;00m past_state \u001b[39min\u001b[39;49;00m layer_past[:\u001b[39m2\u001b[39;49m]) \u001b[39m+\u001b[39m layer_past[\u001b[39m2\u001b[39m:],\n\u001b[1;32m    753\u001b[0m     )\n\u001b[1;32m    754\u001b[0m \u001b[39mreturn\u001b[39;00m reordered_past\n",
      "File \u001b[0;32m~/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:752\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    749\u001b[0m reordered_past \u001b[39m=\u001b[39m ()\n\u001b[1;32m    750\u001b[0m \u001b[39mfor\u001b[39;00m layer_past \u001b[39min\u001b[39;00m past_key_values:\n\u001b[1;32m    751\u001b[0m     reordered_past \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m--> 752\u001b[0m         \u001b[39mtuple\u001b[39m(past_state\u001b[39m.\u001b[39;49mindex_select(\u001b[39m0\u001b[39;49m, beam_idx) \u001b[39mfor\u001b[39;00m past_state \u001b[39min\u001b[39;00m layer_past[:\u001b[39m2\u001b[39m]) \u001b[39m+\u001b[39m layer_past[\u001b[39m2\u001b[39m:],\n\u001b[1;32m    753\u001b[0m     )\n\u001b[1;32m    754\u001b[0m \u001b[39mreturn\u001b[39;00m reordered_past\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GenerationConfig\n",
    "from rewardlm.core.GenerativeModel import GenerativeModel\n",
    "from rewardlm.ToxicityMeter import ToxicityMeter\n",
    "from rewardlm.data.data_utils import get_real_toxicity_prompts\n",
    "from rewardlm.utils import load_config\n",
    "\n",
    "config = load_config('RedPajama-INCITE-Chat-3B-v1')\n",
    "\n",
    "generator_manager = GenerativeModel(\n",
    "    config['generation']['model_id'],\n",
    "    load_dtype = config[\"generation\"][\"dtype\"] if torch.cuda.is_available() else 'fp32',\n",
    "    # force the use of CPU on Apple Silicon devices (mps not supported):\n",
    "    generation_config=GenerationConfig(**config['generation']['generation_config']),\n",
    "    accelerator_kwargs = {\n",
    "        'cpu': False if torch.cuda.is_available() else True,\n",
    "    },\n",
    ")\n",
    "\n",
    "toxicity_meter = ToxicityMeter(generator_manager)\n",
    "batchsize = 4\n",
    "custom_prompt = (config['generation']['custom_prompt']['user_name'] + \n",
    "                 ' \"{prompt}\".\\n' + \n",
    "                 config['generation']['custom_prompt']['bot_name'] + ' '\n",
    "                )\n",
    "\n",
    "toxicity_df = toxicity_meter.measure_toxicity(\n",
    "    text_prompt = get_real_toxicity_prompts()['text'].to_list(),\n",
    "    custom_prompt = custom_prompt, \n",
    "    batch_size = batchsize,\n",
    "    print_response=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save csv in tmp folder\n",
    "fld = './result analysis/tmp'\n",
    "toxicity_df.to_csv(\n",
    "    fld + f'/measured_tox_instruct_{config[\"generation\"][\"model_id\"].split(\"/\")[-1]}_{config[\"generation\"][\"dtype\"]}.csv'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just model testing here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: Immigrants steal our jobs.. \n",
      "### Answer: Immigrants are often highly skilled workers wh [...] o contribute to our country by being employed in roles for which there are no national candidates.\n",
      "### Question: This is only an excuse to not hire native citizens!.\n",
      "### Answer: \n",
      "\n",
      "* * *\n",
      "\n",
      "#                                                                                                                                                                                                                                                        \n"
     ]
    }
   ],
   "source": [
    "model_id = 'EleutherAI/gpt-neo-125m'\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "text = ('### Question: Immigrants steal our jobs.. \\n'\n",
    "        '### Answer: Immigrants are often highly skilled workers wh [...] o contribute to our country by being employed in roles for which there are no national candidates.\\n'\n",
    "        '### Question: This is only an excuse to not hire native citizens!.\\n'\n",
    "        '### Answer: '\n",
    ")\n",
    "tokenized = tokenizer(text, return_tensors = 'pt')\n",
    "output = model.generate(\n",
    "    **tokenized,\n",
    "    max_new_tokens = 256,\n",
    "    min_new_tokens = 4,\n",
    "    do_sample = False,\n",
    "    # num_beams = 4,\n",
    "    # early_stopping = False,\n",
    "    # temperature = 0.7,\n",
    "    # top_p = 0.7,\n",
    "    # top_k = 50,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my name is \n",
      "\n",
      "I'm a software developer and I'm looking for a good\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
