{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## dependencies for colab\n",
    "# !git clone https://__TOKEN_GIT__:@github.com/DanielSc4/RewardLM.git\n",
    "# %cd RewardLM/\n",
    "# !pip install -r requirements.txt\n",
    "# from huggingface_hub import login\n",
    "# login(token = '')  # https://huggingface.co/settings/tokens\n",
    "\n",
    "import os\n",
    "def init():\n",
    "    # disable welcome message\n",
    "    os.environ['BITSANDBYTES_NOWELCOME'] = '1'\n",
    "    # os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "    # os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "def update_code():\n",
    "    !git pull\n",
    "    %cd RewardLM/\n",
    "\n",
    "init()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü•û Reinforcement Learning with Automatic Feedback (RLAF)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation config from [here](https://github.com/LAION-AI/Open-Assistant/blob/main/model/model_training/configs/ppo_config.yaml) seems to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('AlekseyKorshuk/vicuna-7b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8312"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rewardlm.data.data_utils import get_DIALOCONAN_prepro\n",
    "data = get_DIALOCONAN_prepro(delete_last_assistant_response = True)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/home1/p313544/Documents/RewardLM/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from rewardlm.core.RL.RLModel import RLModel\n",
    "from rewardlm.data.data_utils import get_DIALOCONAN_prepro\n",
    "from transformers import GenerationConfig\n",
    "from rewardlm.utils import load_config\n",
    "\n",
    "config = load_config('debug_RL')\n",
    "\n",
    "rlmanager = RLModel(\n",
    "    model_id = config['generation']['model_id'],\n",
    "    reward_model_id = config['reward']['model_id'],\n",
    "    optimized = True,   # use LoRA\n",
    "    bs = config['PPO']['bs'],\n",
    "    mini_bs = config['PPO']['mini_bs'],\n",
    "    # force the use of CPU on Apple Silicon devices (mps not supported):\n",
    "    accelerator_kwargs = {\n",
    "        'cpu': False if torch.cuda.is_available() else True,\n",
    "    },\n",
    "    generation_config=GenerationConfig(\n",
    "        max_new_tokens = 512,\n",
    "        min_new_tokens = 5,\n",
    "        pad_token_id = 0,       # crashes while using batchsize > 1 only on mps device if not set\n",
    "        temperature = 1,\n",
    "        top_p = .7,\n",
    "        top_k = 0,\n",
    "        do_sample = True\n",
    "        # diversity_penalty = .1, # should use num_beam_groups > 1\n",
    "    )\n",
    ")\n",
    "\n",
    "data = get_DIALOCONAN_prepro(delete_last_assistant_response = True)\n",
    "if config['data']['subset']:\n",
    "    # select only the first `subset_size` samples\n",
    "    data = data[:config['data']['subset_size']]\n",
    "dataset = rlmanager.generate_dataset(text = data)\n",
    "\n",
    "stats = rlmanager.train_PPO(dataset = dataset)\n",
    "print('Done')\n",
    "\n",
    "# assuming debug if subset is active\n",
    "if not config['data']['subset']:\n",
    "    # save trainer (model, tokenizer & config) to the hub\n",
    "    repo_id = 'DanielSc4/' + config['generation']['model_id'].split('/')[1] + '-RL-LoRA-test0'\n",
    "\n",
    "    rlmanager.push_generator_to_hub(repo_id = repo_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üë®üèº‚Äçüè´ Model fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /Users/daniel/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "CUDA SETUP: Loading binary /Users/daniel/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n",
      "dlopen(/Users/daniel/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so, 0x0006): tried: '/Users/daniel/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file), '/System/Volumes/Preboot/Cryptexes/OS/Users/daniel/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (no such file), '/Users/daniel/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file)\n",
      "Accelerator selected device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/bitsandbytes/cextension.py:33: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded in fp32 (standard) mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0216a363ead4c71910a6a994b38b0bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1179648 || all params 126378240 || trainable(%): 0.93\n",
      "Trainer device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdanielsc4\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/daniel/Documents/Work/RewardLM/wandb/run-20230623_171405-ua950ub4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/danielsc4/huggingface/runs/ua950ub4' target=\"_blank\">gpt-neo-125m-FT-LoRA-test1</a></strong> to <a href='https://wandb.ai/danielsc4/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/danielsc4/huggingface' target=\"_blank\">https://wandb.ai/danielsc4/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/danielsc4/huggingface/runs/ua950ub4' target=\"_blank\">https://wandb.ai/danielsc4/huggingface/runs/ua950ub4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "482e722aa6954af29b6311e6fe683e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GenerationConfig\n",
    "from rewardlm.data.data_utils import get_DIALOCONAN_prepro, get_dataset_CLM\n",
    "from rewardlm.core.GenerativeModel import GenerativeModel\n",
    "from rewardlm.utils import load_config\n",
    "\n",
    "config = load_config('debug_notGPT_FT')\n",
    "repo_id = 'DanielSc4/' + config['generation']['model_id'].split('/')[1] + '-FT-LoRA-test1'\n",
    "\n",
    "generator_manager = GenerativeModel(\n",
    "    config['generation']['model_id'],\n",
    "    load_dtype = '8-bit' if torch.cuda.is_available() else 'fp32',\n",
    "    # force the use of CPU on Apple Silicon devices (mps not supported):\n",
    "    generation_config=GenerationConfig(**config['generation']['generation_config']),\n",
    "    accelerator_kwargs = {\n",
    "        'cpu': False if torch.cuda.is_available() else True,\n",
    "    },\n",
    ")\n",
    "\n",
    "# download dataset\n",
    "data = get_DIALOCONAN_prepro(**config['generation']['custom_prompt'])\n",
    "if config['data']['subset']:\n",
    "    print('getting subset')\n",
    "    # select only the first `subset_size` samples\n",
    "    data = data[:config['data']['subset_size']]\n",
    "dataset = get_dataset_CLM(\n",
    "    data, \n",
    "    context_length = 512, \n",
    "    tokenizer = generator_manager.tokenizer\n",
    ")\n",
    "\n",
    "generator_manager.fine_tune(\n",
    "    dataset = dataset, \n",
    "    val_set_per=.1,     # 10% of the dataset\n",
    "    optimized = True,   # if torch.cuda.is_available() else False,\n",
    "    lr = config['generation']['lr'],\n",
    "    epochs = config['generation']['epochs'],\n",
    "    initial_bs = config['generation']['initial_bs'],\n",
    "    run_name = repo_id.split('/')[1],\n",
    ")\n",
    "\n",
    "# assuming debug if subset is active\n",
    "if not config['data']['subset']:\n",
    "    # save model to the hub\n",
    "    generator_manager.push_to_hub(repo_id = repo_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the fine tuned model from ü§ó Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerator selected device: mps\n",
      "Obtaining original model: MBZUAI/LaMini-GPT-124M\n",
      "Inference test: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS does not support cumsum op with int64 input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mInference test: \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m inputs \u001b[39m=\u001b[39m new_generator\u001b[39m.\u001b[39mtokenizer(\u001b[39m\"\u001b[39m\u001b[39mUser: Woman should always be considere inferior?\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAssistant: \u001b[39m\u001b[39m\"\u001b[39m, return_tensors \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m new_generator\u001b[39m.\u001b[39;49minference_fine_tuned(tokenized_batch\u001b[39m=\u001b[39;49minputs, return_decoded\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Documents/Work/RewardLM/rewardlm/core/GenerativeModel.py:257\u001b[0m, in \u001b[0;36mGenerativeModel.inference_fine_tuned\u001b[0;34m(self, tokenized_batch, return_decoded)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n\u001b[1;32m    256\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 257\u001b[0m     output_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtokenized_batch, generation_config \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgeneration_config)\n\u001b[1;32m    259\u001b[0m \u001b[39mif\u001b[39;00m return_decoded:\n\u001b[1;32m    260\u001b[0m     output_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mdecode(output_model[\u001b[39m0\u001b[39m], skip_special_tokens \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/peft/peft_model.py:731\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(peft_config, PromptLearningConfig):\n\u001b[0;32m--> 731\u001b[0m         outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    732\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    733\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m~/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/trl/models/modeling_value_head.py:195\u001b[0m, in \u001b[0;36mAutoModelForCausalLMWithValueHead.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    184\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39m    A simple wrapper around the `generate` method of the wrapped model.\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39m    Please refer to the [`generate`](https://huggingface.co/docs/transformers/internal/generation_utils)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39m            Keyword arguments passed to the `generate` method of the wrapped model.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpretrained_model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1522\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1517\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnum_return_sequences has to be 1 when doing greedy search, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1518\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut is \u001b[39m\u001b[39m{\u001b[39;00mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1519\u001b[0m         )\n\u001b[1;32m   1521\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1522\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[1;32m   1523\u001b[0m         input_ids,\n\u001b[1;32m   1524\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1525\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1526\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1527\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1528\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1529\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1530\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1531\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1532\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1533\u001b[0m     )\n\u001b[1;32m   1535\u001b[0m \u001b[39melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[1;32m   1536\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2336\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2333\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   2335\u001b[0m \u001b[39m# prepare model inputs\u001b[39;00m\n\u001b[0;32m-> 2336\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   2338\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m   2339\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[1;32m   2340\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[1;32m   2341\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   2342\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   2343\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   2344\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Work/RewardLM/.venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1025\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.prepare_inputs_for_generation\u001b[0;34m(self, input_ids, past_key_values, inputs_embeds, **kwargs)\u001b[0m\n\u001b[1;32m   1021\u001b[0m position_ids \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mposition_ids\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   1023\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m position_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1024\u001b[0m     \u001b[39m# create position_ids on the fly for batch generation\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m     position_ids \u001b[39m=\u001b[39m attention_mask\u001b[39m.\u001b[39;49mlong()\u001b[39m.\u001b[39;49mcumsum(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1026\u001b[0m     position_ids\u001b[39m.\u001b[39mmasked_fill_(attention_mask \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m   1027\u001b[0m     \u001b[39mif\u001b[39;00m past_key_values:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS does not support cumsum op with int64 input"
     ]
    }
   ],
   "source": [
    "from rewardlm.core.GenerativeModel import GenerativeModel\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "new_generator = GenerativeModel(\n",
    "    model_id=repo_id,\n",
    "    load_from_peft=True,\n",
    ")\n",
    "\n",
    "print('Inference test: ')\n",
    "inputs = new_generator.tokenizer(\"User: Woman should always be considere inferior?\\nAssistant: \", return_tensors = 'pt')\n",
    "\n",
    "new_generator.inference_fine_tuned(tokenized_batch=inputs, return_decoded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Explain how to solve the immigration probelem\n",
      "Assistant: \n",
      "\n",
      "To solve the immigration probelem, you need to follow these steps:\n",
      "\n",
      "1. Identify the source of the probelem.\n",
      "2. Identify the purpose of the probelem.\n",
      "3. Determine the appropriate action to take.\n",
      "\n",
      "Steps to solve the immigration probelem:\n",
      "\n",
      "1. Identify the source of the probelem.\n",
      "2. Identify the purpose of the probelem.\n",
      "3. Determine the appropriate action to take.\n"
     ]
    }
   ],
   "source": [
    "text = 'Explain how to solve the immigration problem'\n",
    "inputs = new_generator.tokenizer(\n",
    "    \"User: {text}\\nAssistant: \".format(text = text), \n",
    "    return_tensors = 'pt'\n",
    ")\n",
    "output = new_generator.inference_fine_tuned(tokenized_batch=inputs, return_decoded=True)\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚öñÔ∏è ToxicityMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerator selected device: cpu\n",
      "Model loaded in fp32 (standard) mode\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GenerationConfig\n",
    "from rewardlm.core.GenerativeModel import GenerativeModel\n",
    "from rewardlm.ToxicityMeter import ToxicityMeter\n",
    "from rewardlm.data.data_utils import get_real_toxicity_prompts\n",
    "from rewardlm.utils import load_config\n",
    "\n",
    "config = load_config(name = 'debug_GPT-neo')\n",
    "\n",
    "generator_manager = GenerativeModel(\n",
    "    config['model_id'],\n",
    "    load_dtype = config[\"generation\"][\"dtype\"] if torch.cuda.is_available() else 'fp32',\n",
    "    # force the use of CPU on Apple Silicon devices (mps not supported):\n",
    "    generation_config=config['generation']['generation_config'],\n",
    "    accelerator_kwargs = {\n",
    "        'cpu': False if torch.cuda.is_available() else True,\n",
    "    },\n",
    ")\n",
    "\n",
    "toxicity_meter = ToxicityMeter(generator_manager)\n",
    "batchsize = 4\n",
    "custom_prompt = (config['generation']['custom_prompt']['user_name'] + \n",
    "                 ' \"{prompt}\".\\n' + \n",
    "                 config['generation']['custom_prompt']['bot_name'] + ' '\n",
    "                )\n",
    "\n",
    "df = get_real_toxicity_prompts()['text'].to_list()\n",
    "toxicity_df = toxicity_meter.measure_toxicity(\n",
    "    text_prompt = df if not config['data']['subset'] else df[:config['data']['subset_size']],\n",
    "    custom_prompt = custom_prompt, \n",
    "    batch_size = batchsize,\n",
    "    print_response=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save csv in tmp folder\n",
    "fldr = './result analysis/tmp'\n",
    "toxicity_df.to_csv(\n",
    "    fldr + f'/measured_tox_instruct_{config[\"generation\"][\"model_id\"].split(\"/\")[-1]}_{config[\"generation\"][\"dtype\"]}.csv'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just model testing here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: Immigrants steal our jobs.. \n",
      "### Answer: Immigrants are often highly skilled workers wh [...] o contribute to our country by being employed in roles for which there are no national candidates.\n",
      "### Question: This is only an excuse to not hire native citizens!.\n",
      "### Answer: \n",
      "\n",
      "* * *\n",
      "\n",
      "#                                                                                                                                                                                                                                                        \n"
     ]
    }
   ],
   "source": [
    "model_id = 'EleutherAI/gpt-neo-125m'\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "text = ('### Question: Immigrants steal our jobs.. \\n'\n",
    "        '### Answer: Immigrants are often highly skilled workers wh [...] o contribute to our country by being employed in roles for which there are no national candidates.\\n'\n",
    "        '### Question: This is only an excuse to not hire native citizens!.\\n'\n",
    "        '### Answer: '\n",
    ")\n",
    "tokenized = tokenizer(text, return_tensors = 'pt')\n",
    "output = model.generate(\n",
    "    **tokenized,\n",
    "    max_new_tokens = 256,\n",
    "    min_new_tokens = 4,\n",
    "    do_sample = False,\n",
    "    # num_beams = 4,\n",
    "    # early_stopping = False,\n",
    "    # temperature = 0.7,\n",
    "    # top_p = 0.7,\n",
    "    # top_k = 50,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my name is \n",
      "\n",
      "I'm a software developer and I'm looking for a good\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPTNeoForCausalLM(\n",
      "      (transformer): GPTNeoModel(\n",
      "        (wte): Embedding(50257, 768)\n",
      "        (wpe): Embedding(2048, 768)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0): GPTNeoBlock(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPTNeoAttention(\n",
      "              (attention): GPTNeoSelfAttention(\n",
      "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=False\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=False\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPTNeoMLP(\n",
      "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): GPTNeoBlock(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPTNeoAttention(\n",
      "              (attention): GPTNeoSelfAttention(\n",
      "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=False\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=False\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPTNeoMLP(\n",
      "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): GPTNeoBlock(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPTNeoAttention(\n",
      "              (attention): GPTNeoSelfAttention(\n",
      "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=False\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=False\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPTNeoMLP(\n",
      "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): GPTNeoBlock(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPTNeoAttention(\n",
      "              (attention): GPTNeoSelfAttention(\n",
      "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=False\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=False\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPTNeoMLP(\n",
      "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): GPTNeoBlock(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPTNeoAttention(\n",
      "              (attention): GPTNeoSelfAttention(\n",
      "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=False\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=False\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPTNeoMLP(\n",
      "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): GPTNeoBlock(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPTNeoAttention(\n",
      "              (attention): GPTNeoSelfAttention(\n",
      "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=False\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=False\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPTNeoMLP(\n",
      "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): GPTNeoBlock(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPTNeoAttention(\n",
      "              (attention): GPTNeoSelfAttention(\n",
      "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=False\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=False\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPTNeoMLP(\n",
      "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): GPTNeoBlock(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPTNeoAttention(\n",
      "              (attention): GPTNeoSelfAttention(\n",
      "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=False\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=False\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPTNeoMLP(\n",
      "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): GPTNeoBlock(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPTNeoAttention(\n",
      "              (attention): GPTNeoSelfAttention(\n",
      "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=False\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=False\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPTNeoMLP(\n",
      "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): GPTNeoBlock(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPTNeoAttention(\n",
      "              (attention): GPTNeoSelfAttention(\n",
      "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=False\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=False\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPTNeoMLP(\n",
      "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): GPTNeoBlock(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPTNeoAttention(\n",
      "              (attention): GPTNeoSelfAttention(\n",
      "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=False\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=False\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPTNeoMLP(\n",
      "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): GPTNeoBlock(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPTNeoAttention(\n",
      "              (attention): GPTNeoSelfAttention(\n",
      "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "                (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=False\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=False\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPTNeoMLP(\n",
      "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "peft_model_id = \"DanielSc4/gpt-neo-125m-RL-LoRA-8bit-test1\"\n",
    "\n",
    "config = LoraConfig.from_pretrained(peft_model_id)\n",
    "original_pretrained = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "peft_model = get_peft_model(model = original_pretrained, peft_config=config)\n",
    "print(peft_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/Documents/Work/RewardLM/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['attention',\n",
       " 'discretized_integrated_gradients',\n",
       " 'occlusion',\n",
       " 'layer_deeplift',\n",
       " 'gradient_shap',\n",
       " 'layer_gradient_x_activation',\n",
       " 'deeplift',\n",
       " 'layer_integrated_gradients',\n",
       " 'saliency',\n",
       " 'input_x_gradient',\n",
       " 'integrated_gradients',\n",
       " 'lime']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inseq\n",
    "inseq.list_feature_attribution_methods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingfaceDecoderOnlyModel(\n",
       "  (model): GPTNeoForCausalLM(\n",
       "    (transformer): GPTNeoModel(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(2048, 768)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0): GPTNeoBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPTNeoAttention(\n",
       "            (attention): GPTNeoSelfAttention(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPTNeoMLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): GPTNeoBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPTNeoAttention(\n",
       "            (attention): GPTNeoSelfAttention(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPTNeoMLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): GPTNeoBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPTNeoAttention(\n",
       "            (attention): GPTNeoSelfAttention(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPTNeoMLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): GPTNeoBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPTNeoAttention(\n",
       "            (attention): GPTNeoSelfAttention(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPTNeoMLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): GPTNeoBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPTNeoAttention(\n",
       "            (attention): GPTNeoSelfAttention(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPTNeoMLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): GPTNeoBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPTNeoAttention(\n",
       "            (attention): GPTNeoSelfAttention(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPTNeoMLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): GPTNeoBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPTNeoAttention(\n",
       "            (attention): GPTNeoSelfAttention(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPTNeoMLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): GPTNeoBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPTNeoAttention(\n",
       "            (attention): GPTNeoSelfAttention(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPTNeoMLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): GPTNeoBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPTNeoAttention(\n",
       "            (attention): GPTNeoSelfAttention(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPTNeoMLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): GPTNeoBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPTNeoAttention(\n",
       "            (attention): GPTNeoSelfAttention(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPTNeoMLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): GPTNeoBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPTNeoAttention(\n",
       "            (attention): GPTNeoSelfAttention(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPTNeoMLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): GPTNeoBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPTNeoAttention(\n",
       "            (attention): GPTNeoSelfAttention(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPTNeoMLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model_id = \"DanielSc4/gpt-neo-125m-RL-LoRA-8bit-test1\"\n",
    "\n",
    "config = LoraConfig.from_pretrained(peft_model_id)\n",
    "# TODO: come usare un modello peft (?)\n",
    "seq_model = inseq.load_model(config.base_model_name_or_path, attribution_method='input_x_gradient')\n",
    "seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Unused arguments during attribution: {'aggregate_target': True}\n",
      "Attributing with input_x_gradient...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 13.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/><b>0th instance:</b><br/>\n",
       "<html>\n",
       "<div id=\"usieyiccwpusnzjovqwp_viz_container\">\n",
       "    <div id=\"usieyiccwpusnzjovqwp_content\" style=\"padding:15px;border-style:solid;margin:5px;\">\n",
       "        <div id = \"usieyiccwpusnzjovqwp_saliency_plot_container\" class=\"usieyiccwpusnzjovqwp_viz_container\" style=\"display:block\">\n",
       "            \n",
       "<div id=\"qahftrxckafnafqofpva_saliency_plot\" class=\"qahftrxckafnafqofpva_viz_content\">\n",
       "    <div style=\"margin:5px;font-family:sans-serif;font-weight:bold;\">\n",
       "        <span style=\"font-size: 20px;\">Target Saliency Heatmap</span>\n",
       "        <br>\n",
       "        x: Generated tokens, y: Attributed tokens\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" cellpadding=\"5\" cellspacing=\"5\"\n",
       "    style=\"overflow-x:scroll;display:block;\">\n",
       "    <tr><th></th>\n",
       "<th>ƒä</th><th>in</th><th>ƒ†the</th><th>ƒ†United</th><th>ƒ†States</th><th>ƒ†Air</th><th>ƒ†Force</th><th>.</th><th>ƒ†</th><th>ƒä</th></tr><tr><th>I</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.1959595959595959)\">0.076</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.2826698356110118)\">0.109</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.14866310160427795)\">0.058</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.1171321053673995)\">0.048</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.10136660724896006)\">0.041</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.10136660724896006)\">0.041</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.09348385818974037)\">0.036</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.08560110913052081)\">0.034</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.24325609031491383)\">0.094</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.14866310160427795)\">0.06</th></tr><tr><th>ƒ†am</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.18019409784115661)\">0.07</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.1880768469003763)\">0.074</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.1171321053673995)\">0.048</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.10924935630817992)\">0.043</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.06983561101208159)\">0.028</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.07771836007130124)\">0.03</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.04618736383442265)\">0.021</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.06195286195286207)\">0.025</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.12501485442661908)\">0.05</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.08560110913052081)\">0.035</th></tr><tr><th>ƒ†Daniel</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.14866310160427795)\">0.059</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.13289760348583876)\">0.053</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.219607843137255)\">0.086</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.2117250940780353)\">0.084</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.1171321053673995)\">0.046</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.10136660724896006)\">0.042</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.04618736383442265)\">0.019</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.05407011289364243)\">0.023</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.14866310160427795)\">0.059</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.07771836007130124)\">0.032</th></tr><tr><th>ƒ†and</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.2747870865517925)\">0.106</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.2117250940780353)\">0.084</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.15654585066349747)\">0.061</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.16442859972271742)\">0.066</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.14078035254505847)\">0.057</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.1171321053673995)\">0.046</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.09348385818974037)\">0.036</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.10136660724896006)\">0.042</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.15654585066349747)\">0.061</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.10924935630817992)\">0.045</th></tr><tr><th>ƒ†I</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.1171321053673995)\">0.045</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.09348385818974037)\">0.039</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.09348385818974037)\">0.038</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.06195286195286207)\">0.026</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.04618736383442265)\">0.021</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.05407011289364243)\">0.022</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.02253911665676371)\">0.011</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.030421865715983164)\">0.013</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.06195286195286207)\">0.026</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.06195286195286207)\">0.025</th></tr><tr><th>ƒ†work</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.1880768469003763)\">0.074</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.18019409784115661)\">0.071</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.1880768469003763)\">0.073</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.15654585066349747)\">0.063</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.07771836007130124)\">0.031</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.1171321053673995)\">0.047</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.04618736383442265)\">0.02</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.05407011289364243)\">0.023</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.07771836007130124)\">0.032</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.06983561101208159)\">0.028</th></tr><tr><th>ƒ†as</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.08560110913052081)\">0.035</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.07771836007130124)\">0.033</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.07771836007130124)\">0.031</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.07771836007130124)\">0.031</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.04618736383442265)\">0.019</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.04618736383442265)\">0.02</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.02253911665676371)\">0.011</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.02253911665676371)\">0.011</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.03830461477520289)\">0.017</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.05407011289364243)\">0.021</th></tr><tr><th>ƒ†a</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.13289760348583876)\">0.053</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.1171321053673995)\">0.047</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.06983561101208159)\">0.028</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.09348385818974037)\">0.037</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.07771836007130124)\">0.032</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.06195286195286207)\">0.025</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.03830461477520289)\">0.016</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.03830461477520289)\">0.018</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.07771836007130124)\">0.033</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.09348385818974037)\">0.038</th></tr><tr><th>ƒ†pilot</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.77928302634185)\">0.3</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.5900970489205783)\">0.228</th><th style=\"background:rgba(255.0, 13.0, 87.0, 1.0)\">0.387</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.9842345018815607)\">0.379</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.8975242622301447)\">0.345</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.7398692810457518)\">0.284</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.43244206773618543)\">0.168</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.2511388393741335)\">0.098</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.4245593186769657)\">0.165</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.2590215884333532)\">0.102</th></tr><tr><th>ƒ†</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.46397306397306415)\">0.181</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.24325609031491383)\">0.096</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.05407011289364243)\">0.023</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.06195286195286207)\">0.025</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.06195286195286207)\">0.025</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.03830461477520289)\">0.017</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.04618736383442265)\">0.021</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.03830461477520289)\">0.016</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.29055258467023165)\">0.115</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.18019409784115661)\">0.071</th></tr><tr><th>ƒä</th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.4245593186769657)\">0.166</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.14866310160427795)\">0.058</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.14866310160427795)\">0.06</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.1171321053673995)\">0.048</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.10924935630817992)\">0.045</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.10136660724896006)\">0.04</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.13289760348583876)\">0.052</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.2117250940780353)\">0.084</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.4718558130322837)\">0.183</th></tr><tr><th>in</th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.2826698356110118)\">0.109</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.1880768469003763)\">0.075</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.08560110913052081)\">0.035</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.14866310160427795)\">0.058</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.06983561101208159)\">0.029</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.10924935630817992)\">0.044</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.04618736383442265)\">0.02</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.02253911665676371)\">0.009</th></tr><tr><th>ƒ†the</th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.15654585066349747)\">0.062</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.06983561101208159)\">0.028</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.06195286195286207)\">0.027</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.08560110913052081)\">0.033</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.06195286195286207)\">0.025</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.030421865715983164)\">0.013</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.06195286195286207)\">0.025</th></tr><tr><th>ƒ†United</th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.6295107942166767)\">0.244</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.306318082788671)\">0.119</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.2747870865517925)\">0.107</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.16442859972271742)\">0.066</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.10136660724896006)\">0.042</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.10136660724896006)\">0.04</th></tr><tr><th>ƒ†States</th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.4560903149138443)\">0.176</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.29843533372945136)\">0.116</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.20384234501881549)\">0.079</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.09348385818974037)\">0.037</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.10136660724896006)\">0.04</th></tr><tr><th>ƒ†Air</th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.8108140225787285)\">0.313</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.5191523073876015)\">0.2</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.10924935630817992)\">0.043</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.14866310160427795)\">0.06</th></tr><tr><th>ƒ†Force</th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.5979797979797981)\">0.232</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.10924935630817992)\">0.045</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.1171321053673995)\">0.047</th></tr><tr><th>.</th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.16442859972271742)\">0.066</th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.14078035254505847)\">0.057</th></tr><tr><th>ƒ†</th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(255.0, 13.0, 87.0, 0.20384234501881549)\">0.08</th></tr><tr><th>ƒä</th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th><th style=\"background:rgba(0.0, 0.0, 0.0, 0.0)\"></th></tr></table>\n",
       "</div>\n",
       "\n",
       "        </div>\n",
       "    </div>\n",
       "</div>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = seq_model.attribute(\n",
    "    'I am Daniel and I work as a pilot ',\n",
    "    # attribute_target=True,      # Specific to encoder-decoder models\n",
    "    # step_scores=[\"probability\"],\n",
    "    show_progress = True,\n",
    "    aggregate_target = True,\n",
    ")\n",
    "out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/Users/daniel/Documents/Work/RewardLM/.venv/lib/python3.9/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Unused arguments during attribution: {'aggregate_target': False}\n",
      "Attributing with input_x_gradient...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 13.95it/s]\n"
     ]
    }
   ],
   "source": [
    "out2 = seq_model.attribute(\n",
    "    'I am Daniel and I work as a pilot ',\n",
    "    # attribute_target=True,      # Specific to encoder-decoder models\n",
    "    # step_scores=[\"probability\"],\n",
    "    show_progress = True,\n",
    "    aggregate_target = False,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
